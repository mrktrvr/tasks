{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Power Consumption Prediction \n",
    "\n",
    "## Overview\n",
    "### The task\n",
    "to estimate hourly power consumptions of today from the power consumptions history until today.\n",
    "\n",
    "At 12 AM each day, forecasting module provides the estimated power consumptions of each hour on the day. \n",
    "\n",
    "Power Consumption Prediction is a task of regressions. Essentially, we can feed power consumption data in the past and features to the models to train and estimate.\n",
    "\n",
    "Requirements are high accurate estimation. Also, computing speed may be required to provide estimation because faster computing allows to use as nearer data as possible to accurate estimation.\n",
    "\n",
    "### Model\n",
    "- Linear Regression\n",
    "\n",
    "We used Linear Regression which is one of the most basic regression techniques.\n",
    "We can also try Gaussian Process regression, Support Vector Regression, or any other regression models. Those models may give better performance however they require some tunings and computational cost. On the other hand Linear regression is very simple but it is very fast and tuning is not required. Also Linear regression can be a baseline of further improvement.\n",
    "\n",
    "### Features\n",
    "- power consumptions of previous 7 days from the target day\n",
    "- hourly differences of power consumptions of previous 7 days from the target day\n",
    "\n",
    "In terms of the data and features, we only used power consumption data to make them simple. We trained the models to estimate consumptions in a day by using previous 7 days data. Also, as extra features, we put difference between each hour and the previous hour to extract the daily trends.\n",
    "This feature design allows to predict hour by hour by using like sliding windows.\n",
    "\n",
    "### Overall test accuracy\n",
    "#### Root Mean Square Error\n",
    "\n",
    "798.7906\n",
    "\n",
    "$\\mathrm{RMSE} = \\sqrt{\\frac{1}{N}\\sum_{n=1}^{N}(truth - estim)^2}$\n",
    "\n",
    "#### Match Rate (rate of overlapping region between truth and estimation)\n",
    "\n",
    "0.9586\n",
    "<img src=\"match_rate_example.png\" width=\"200\">\n",
    "\n",
    "### Computational speed\n",
    "Both training and prediction take less than 1 second.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots prediction and truth\n",
    "Each plot shows the day by day estimation result (blue line), truth (red line) and input (light coloured lines). \n",
    "\n",
    "Surprisingly, the estimations are more accurate than we thought, especially daily trends (peaks and bottoms). However these are not perfect enough. To improve the performance, we can add some features such as weather information and seasonally trends. Also we can try more complicated models.\n",
    "\n",
    "Estimations on Mondays, Saturdays, and Sundays seemed to be less accurate. The reason is that power consumptions on those days are different from the previous days. Modeling on each week day would solve this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import plotter\n",
    "\n",
    "sys.path.append(os.path.join('..', 'hec'))\n",
    "import utils.data_handler as dh\n",
    "from utils.logger import logger\n",
    "import model.preprocessing as pp\n",
    "import model.model as mdl\n",
    "from utils.eval import match_rate\n",
    "logger.setLevel('INFO')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_dir = '../data/'\n",
    "n_data_to_load = 2\n",
    "data = dh.data_loader(data_dir, n_data_to_load)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data to feature vectors and target vectors\n",
    "Training and testing data were divided by the ratio of 7 : 3. First 70% of the time series data was assigned for training and the rest of them was for testing.\n",
    "\n",
    "feat_train and feat_test are the input to estimate the target values tar_train and tar_test.\n",
    "As the features, we set power consumptions of 7 days before the target and the differences during the 7 days.\n",
    "They have shapes of (data_length, feature_length). The feature_length is 24 hours * 7 days + 24 hours * 7 days - 1.\n",
    "tar_train and tar_test have shapes of (data_length, 24 hours).\n",
    "\n",
    "feat_train and tar_train were used for training the models. feat_test was used for estimation, and tar_test was used for checking the estimation results.\n",
    "\n",
    "There are also some preprocessing. Power consumptions are scaled between 0 and 1. To make the structured data, if the data started from the hours which are not 12 AM, some data before 0 AM were removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = list(data.values())[1]\n",
    "n_days = 7\n",
    "norm_prm_file_name = 'norm_prm_%s.pkl' % list(data.keys())[1]\n",
    "ret = pp.data_preparation(df, norm_prm_file_name, n_days)\n",
    "feat_train, tar_train, feat_test, tar_test, idx_train, idx_test = ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "Linear regression is one of the most simple regression model. Before using more complicated models, we should see the estimation results of simpler models. We can obtain the estimation results to execute the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "l_r = mdl.Model()\n",
    "l_r.fit(feat_train, tar_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "l_r_model = l_r.get_model()\n",
    "print('train score:', l_r_model.score(feat_train, tar_train))\n",
    "print('test score :',l_r_model.score(feat_test, tar_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction from trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pred_train = l_r.predict(feat_train)\n",
    "pred_test = l_r.predict(feat_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimation results from Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## closed evaluation (training v estimation from training) data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data for plotting\n",
    "data is put into DataFrame again for visualisations. Inverse normalisation also applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = pp.post_process_array_to_df(feat_train, pred_train, tar_train, n_days, idx_train)\n",
    "df_feat_nrm_trn, df_pred_nrm_trn, df_tar_nrm_trn = ret\n",
    "ret = pp.post_process_inverse_norm(df_feat_nrm_trn, df_pred_nrm_trn, df_tar_nrm_trn, norm_prm_file_name)\n",
    "df_feat_trn, df_pred_trn, df_tar_trn = ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day by day Root Mean Square Error (RMSE)\n",
    "The plot below illustrates day by day root mean square between truth and estimations. x-axis is the days and y-axis has RMSE. RMSE gives the distance between truth and prediction which means smaller values are the better.\n",
    "\n",
    "$\\mathrm{RMSE} = \\sqrt{\\frac{1}{N}\\sum_{n=1}^{N}(truth - estim)^2}$\n",
    "\n",
    "The RMSE at middle of the graph is higher than the other area. This may be the reason that features of this model is based on previous 7 days and on these days, temperatures might have changed significantly. For further investigation, some extra information such as weather histories may be required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plotter.plot_rmse_day_by_day(df_tar_nrm_trn, df_pred_nrm_trn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall RMSE and Match rate\n",
    "Since RMSE gives the distance between truth and prediction. It is good for comparison in same scale but it is difficult to see absolute performance. So, we introduced Match rate.\n",
    "\n",
    "Match rate gives the rate how much truth and prediction overlap each other.\n",
    "```\n",
    "upper = max([truth, estim])\n",
    "lower = min([truth, estim])\n",
    "match_rate = lower.sum() / upper.sum()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('RMSE(normalised):', np.sqrt(MSE(df_tar_nrm_trn.values.flatten(), df_pred_nrm_trn.values.flatten())))\n",
    "print('RMSE            :', np.sqrt(MSE(df_tar_trn.values.flatten(), df_pred_trn.values.flatten())))\n",
    "print('Match Rate      :', match_rate(df_tar_trn.values.flatten(), df_pred_trn.values.flatten()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots Training data and closed prediction\n",
    "Each plot shows the day by day estimation result (blue line), truth (red line) and input (light coloured lines). \n",
    "\n",
    "Surprisingly, the estimations are more accurate than we thought, especially daily trends (peaks and bottoms). However these are not perfect enough. To improve the performance, we can add some features such as weather information and seasonally trends. Also we can try more complicated models.\n",
    "\n",
    "Estimations on Mondays, Saturdays, and Sundays seemed to be less accurate. The reason is that power consumptions on those days are different from the previous days. Modeling on each week day would solve this issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plotter.plot_regression_res(df_feat_trn, df_tar_trn, df_pred_trn, 7, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## open evaluation (testing v estimation from testing data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ret = pp.post_process_array_to_df(feat_test, pred_test, tar_test, n_days, idx_test)\n",
    "df_feat_nrm_tst, df_pred_nrm_tst, df_tar_nrm_tst = ret\n",
    "ret = pp.post_process_inverse_norm(df_feat_nrm_tst, df_pred_nrm_tst, df_tar_nrm_tst, norm_prm_file_name)\n",
    "df_feat_tst, df_pred_tst, df_tar_tst = ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plotter.plot_rmse_day_by_day(df_tar_nrm_tst, df_pred_nrm_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('RMSE(normalised):', np.sqrt(MSE(df_tar_nrm_tst.values.flatten(), df_pred_nrm_tst.values.flatten())))\n",
    "print('RMSE            :', np.sqrt(MSE(df_tar_tst.values.flatten(), df_pred_tst.values.flatten())))\n",
    "print('Match Rate      :', match_rate(df_tar_tst.values.flatten(), df_pred_tst.values.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plotter.plot_regression_res(df_feat_tst, df_tar_tst, df_pred_tst, 7, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further improvement and feature work\n",
    "Mondays, Saturdays, and Sundays were not modeled well. Also, the estimations were highly related on previous days. On the other hand, as we discussed in analysis section, there are seasonally trends, weekly trends and daily trends. To model using these information, better feature design is required.\n",
    "\n",
    "Mondays, Saturdays, and Sundays might be modeled individually, however, this approach can not use the information of previous days. So some methods to intentionally introduce trends may be required. The trends can be treated as prior information. To introduce prior information such as daily trends, we may use Bayesian inference. Bayesian approaches also allow to update the model.\n",
    "\n",
    "Also, we haven't handled some extra cases such as extreme high power consuming, caused by weather conditions and some events like Super bowl.\n",
    "To handle these cases and Saturday and Sunday problem, we may want to have some uncertainty to have decisions. Bayesian models or Gaussian processes can give such information. These approaches are worth trying if there is enough time.\n",
    "\n",
    "Moreover, some external information like temperatures would help. To do this, more time and resources may be required.\n",
    "\n",
    "For the use case of energy generation, shortage is worse than excess. In order to avoid this, some bias can be given in post process. Also we need to introduce some criteria to evaluate how much and often and shortage happens.\n",
    "\n",
    "For the use case of energy trading, we can advice that we should be conservative to trade weekends and Mondays."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
